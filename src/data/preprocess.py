import os
import pickle
import re
from typing import Generator, List, Tuple, Dict
from collections import Counter

import numpy as np
import nltk
try:
    nltk.download("stopwords", quiet=True)
    nltk.download("punkt", quiet=True) 
    from nltk.tokenize import word_tokenize
    from nltk.corpus import stopwords
except ImportError:
    print("NLTK is not installed. Using simple split() for tokenization.")
    def word_tokenize(text):
        return re.findall(r"\b\w+\b", text) 
    stopwords = set()

# -----------------------------
# ê²½ë¡œ ì„¤ì •
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
SRC_DIR = os.path.dirname(SCRIPT_DIR)
ROOT_DIR = os.path.dirname(SRC_DIR)

CORPUS_PATH = os.path.join(ROOT_DIR, "data/pretrain/raw/elmo_corpus.txt")
TOKENIZED_TRAIN_PATH = os.path.join(ROOT_DIR, "data/pretrain/tokenized_corpus.txt")
TOKEN_INDICES_PATH = os.path.join(ROOT_DIR, "data/pretrain/token_indices.npy")

# ELMo íŠ¹ìˆ˜ í† í°
PAD_TOKEN = "<PAD>"
UNK_TOKEN = "<UNK>"
BOS_TOKEN = "<S>"
EOS_TOKEN = "</S>"

STOPWORDS = set(stopwords.words('english')) if 'stopwords' in locals() and stopwords else set()

# -----------------------------
def preprocess_tokens(tokens: list):
    """í† í° ë¦¬ìŠ¤íŠ¸ ì „ì²´ ì „ì²˜ë¦¬"""
    # clean_token ì œê±°: í† í°ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜
    return [t for t in tokens if t]

# -----------------------------
def preprocess_text(text: str) -> list:
    """ë‹¨ì¼ ë¬¸ì„œ í…ìŠ¤íŠ¸ì— ëŒ€í•´ ì „ì²˜ë¦¬ ë° í† í°í™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."""

    text = re.sub(r'==\s*(References|External links|See also|Notes|Sources)\s*==.*', '', text, flags=re.DOTALL | re.IGNORECASE)
    
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    
    tokens = word_tokenize(text)

    return preprocess_tokens(tokens)

# -----------------------------
def process_corpus_and_stream(path=CORPUS_PATH) -> Generator[List[str], None, None]:
    """
    ì›ë³¸ íŒŒì¼ì„ í•œ ì¤„ì”© ì½ì–´ ë¬¸ì„œ('\n\n'ìœ¼ë¡œ êµ¬ë¶„)ë¥¼ ì¬êµ¬ì„±í•˜ê³ , 
    ì „ì²˜ë¦¬ ë° í† í°í™”ëœ í† í° ë¦¬ìŠ¤íŠ¸(ë¬¸ì¥/ë¬¸ë§¥ ë‹¨ìœ„)ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ yield
    """
    if not os.path.exists(path):
        raise FileNotFoundError(f"Corpus file not found: {path}.")
    
    print(f"Starting streaming process from {path}.")
    
    doc_buffer = []
    doc_count = 0
    
    with open(path, "r", encoding="utf-8", errors="ignore") as f:
        for line in f:
            line = line.strip()
            
            if line:
                doc_buffer.append(line)
                
            if not line and doc_buffer:
                doce_text = " ".join(doc_buffer)
                
                tokens = preprocess_text(doce_text)
                
                if tokens:
                    yield tokens 
                    doc_count += 1
                    
                doc_buffer = []
                
                if doc_count % 100000 == 0 and doc_count > 0:
                    print(f"Processed {doc_count:,} documents so far...")
                    
        if doc_buffer:
            doce_text = " ".join(doc_buffer)
            tokens = preprocess_text(doce_text)
            if tokens:
                yield tokens 
                doc_count += 1
    print(f"\nProcessing complete. Total documents processd: {doc_count:,}")


def build_vocab_stream(
    file_path: str,
    min_count: int = 1
) -> Tuple[List[str], Dict[str, int], Dict[int, str], Dict[str, int]]:
    """
    í† í°í™”ëœ íŒŒì¼ì„ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì½ì–´ vocabì„ êµ¬ì¶•í•©ë‹ˆë‹¤.
    
    Args:
        file_path: í† í°í™”ëœ í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ (í•œ ì¤„ì— ê³µë°±ìœ¼ë¡œ êµ¬ë¶„ëœ í† í°ë“¤)
        min_count: vocabì— í¬í•¨ë  ìµœì†Œ ë¹ˆë„ìˆ˜
    
    Returns:
        vocab: ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ (íŠ¹ìˆ˜ í† í° í¬í•¨)
        word2idx: ë‹¨ì–´ -> ì¸ë±ìŠ¤ ë§¤í•‘
        idx2word: ì¸ë±ìŠ¤ -> ë‹¨ì–´ ë§¤í•‘
        word_freq: ë‹¨ì–´ ë¹ˆë„ ë”•ì…”ë„ˆë¦¬
    """
    print(f"Building vocabulary from {file_path}...")
    
    word_counter = Counter()
    total_lines = 0
    
    # 1. ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
    with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
        for line in f:
            tokens = line.strip().split()
            word_counter.update(tokens)
            total_lines += 1
            if total_lines % 100000 == 0:
                print(f"  Processed {total_lines:,} lines...")
    
    print(f"Total unique tokens (before filtering): {len(word_counter):,}")
    
    # 2. min_count ì´ìƒì¸ ë‹¨ì–´ë§Œ ì„ íƒ
    filtered_words = {word: count for word, count in word_counter.items() 
                     if count >= min_count}
    print(f"Tokens with count >= {min_count}: {len(filtered_words):,}")
    
    # 3. íŠ¹ìˆ˜ í† í° ì¶”ê°€ (ELMo í•™ìŠµì— í•„ìš”)
    # ìˆœì„œ: PAD(0), UNK(1), BOS(2), EOS(3), ì¼ë°˜ ë‹¨ì–´ë“¤(4~)
    vocab = [PAD_TOKEN, UNK_TOKEN, BOS_TOKEN, EOS_TOKEN]
    vocab.extend(sorted(filtered_words.keys()))
    
    # 4. word2idx, idx2word ìƒì„±
    word2idx = {word: idx for idx, word in enumerate(vocab)}
    idx2word = {idx: word for word, idx in word2idx.items()}
    
    # 5. word_freq ìƒì„± (íŠ¹ìˆ˜ í† í° ì œì™¸í•˜ê³  ì‹¤ì œ ë‹¨ì–´ë§Œ)
    word_freq = {word: count for word, count in word_counter.items() 
                if word in word2idx and word not in [PAD_TOKEN, UNK_TOKEN, BOS_TOKEN, EOS_TOKEN]}
    
    print(f"âœ… Vocabulary built: {len(vocab):,} tokens (including special tokens)")
    print(f"   Special tokens: {PAD_TOKEN}(0), {UNK_TOKEN}(1), {BOS_TOKEN}(2), {EOS_TOKEN}(3)")
    
    return vocab, word2idx, idx2word, word_freq


def save_token_indices_to_binary(
    token_stream: Generator[List[str], None, None],
    word2idx: dict,
    save_path=TOKEN_INDICES_PATH
):
    """
    í† í° ìŠ¤íŠ¸ë¦¼ì„ ì¸ë±ìŠ¤ë¡œ ë³€í™˜í•˜ì—¬ ë°”ì´ë„ˆë¦¬ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    vocabì— ì—†ëŠ” í† í°ì€ UNK í† í°ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    print(f"Indexing corpus and saving to {save_path}...")

    all_indices = []
    total_tokens_count = 0
    unk_count = 0
    
    unk_idx = word2idx.get(UNK_TOKEN, 1)
    
    for tokens in token_stream:
        indices = []
        for token in tokens:
            if token in word2idx:
                indices.append(word2idx[token])
            else:
                indices.append(unk_idx)
                unk_count += 1
        all_indices.extend(indices)
        
        total_tokens_count += len(indices)
        if total_tokens_count % 50000000 == 0 and total_tokens_count > 0:
            print(f"Tokens indexed so far: {total_tokens_count:,} (UNK: {unk_count:,})")
    
    token_indices_array = np.array(all_indices, dtype=np.int32)
    
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    np.save(save_path, token_indices_array)
    
    print(f"\nâœ… Corpus indexing complete.")
    print(f"   Total indices: {len(token_indices_array):,}")
    print(f"   UNK tokens: {unk_count:,} ({unk_count/len(token_indices_array)*100:.2f}%)")
    print(f"   Saved to {save_path}")

if __name__ == "__main__":
    
    # ----------------------------- 1. Vocab êµ¬ì¶• ë° ì„ì‹œ íŒŒì¼ ìƒì„± (NS/HS ê³µí†µ) -----------------------------
    try:
        # A. ì›ë³¸ ì½”í¼ìŠ¤ë¥¼ ì½ì–´ í† í°í™”ëœ ë‚´ìš©ì„ ì„ì‹œ íŒŒì¼ì— ì €ì¥ (Vocab êµ¬ì¶•ìš©)
        print(f"Saving temporary tokenized corpus to {TOKENIZED_TRAIN_PATH} for vocab building...")
        total_temp_tokens = 0
        os.makedirs(os.path.dirname(TOKENIZED_TRAIN_PATH), exist_ok=True)
        with open(TOKENIZED_TRAIN_PATH, "w", encoding="utf-8") as f:
            temp_stream = process_corpus_and_stream(CORPUS_PATH)
            for tokens in temp_stream:
                f.write(" ".join(tokens) + "\n")
                total_temp_tokens += len(tokens)
        print(f"Temporary tokenized file created. Total tokens: {total_temp_tokens}")
        
        # B. ì„ì‹œ íŒŒì¼ë¡œ Vocab êµ¬ì¶•
        VOCAB_MIN_COUNT = 3 # config ê°’ì„ ê°€ì •
        vocab, word2idx, idx2word, word_freq = build_vocab_stream(
            TOKENIZED_TRAIN_PATH,
            min_count=VOCAB_MIN_COUNT
        )
        
        # ----------------------------- 2. Vocab íŒŒì¼ ì €ì¥ (NS/HS ê³µí†µ) -----------------------------
        # Vocab íŒŒì¼ ì €ì¥: train.pyê°€ ë¡œë“œí•  ìˆ˜ ìˆë„ë¡ ì €ì¥í•©ë‹ˆë‹¤.
        vocab_data = {
            "vocab": vocab, 
            "word2idx": word2idx, 
            "idx2word": idx2word, 
            "word_freq": word_freq,
            "vocab_size": len(vocab),
            "special_tokens": {
                "PAD": PAD_TOKEN,
                "UNK": UNK_TOKEN,
                "BOS": BOS_TOKEN,
                "EOS": EOS_TOKEN
            }
        }
        vocab_filename = "vocab.pkl"
        vocab_path = os.path.join(ROOT_DIR, "data/pretrain", vocab_filename)
        os.makedirs(os.path.dirname(vocab_path), exist_ok=True)
        
        with open(vocab_path, "wb") as f:
            pickle.dump(vocab_data, f)
        print(f"âœ… Final Vocab saved to {vocab_path}")
        print(f"   Vocab size: {len(vocab):,}")
        print(f"   Word frequency stats: min={min(word_freq.values()) if word_freq else 0}, "
              f"max={max(word_freq.values()) if word_freq else 0}")
        
        # ----------------------------- 3. í•™ìŠµ ì¸ë±ìŠ¤ ìƒì„± ë° ì €ì¥ -----------------------------
        # Vocab êµ¬ì¶•ì„ ìœ„í•´ ì‚¬ìš©í•œ ìŠ¤íŠ¸ë¦¼ì€ ì†Œì§„ë˜ì—ˆìœ¼ë¯€ë¡œ, ìƒˆ ìŠ¤íŠ¸ë¦¼ ìƒì„±
        print("\n" + "="*60)
        print("Starting token indexing for training...")
        print("="*60)
        final_token_stream = process_corpus_and_stream(CORPUS_PATH) 
        save_token_indices_to_binary(final_token_stream, word2idx, TOKEN_INDICES_PATH)
        
        # ----------------------------- 4. ì„ì‹œ íŒŒì¼ ì‚­ì œ (ìœ ì§€) -----------------------------
        if os.path.exists(TOKENIZED_TRAIN_PATH):
            os.remove(TOKENIZED_TRAIN_PATH) # ğŸ‘ˆ ì´ íŒŒì¼ì€ ì´ì œ í•„ìš” ì—†ìœ¼ë¯€ë¡œ ì‚­ì œ
            print(f"ğŸ§¹ Removed temporary file: {TOKENIZED_TRAIN_PATH}")
            
    except FileNotFoundError as e:
        print(f"ì˜¤ë¥˜: {e}")
    except Exception as e:
        print(f"ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")