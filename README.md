# ELMo êµ¬í˜„ (PyTorch)

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.5.1-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

ì–‘ë°©í–¥ LSTM ì–¸ì–´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ELMo (Embeddings from Language Models) PyTorch êµ¬í˜„ì…ë‹ˆë‹¤.

## íŠ¹ì§•

- **Depth 1, 2 ì§€ì›**: 1ì¸µ ë˜ëŠ” 2ì¸µ ì–‘ë°©í–¥ LSTM ì§€ì›
- **Forward & Backward LSTM**: ì–‘ë°©í–¥ ì–¸ì–´ ëª¨ë¸ í•™ìŠµ
- **ë ˆì´ì–´ë³„ ì„ë² ë”© ì¶”ì¶œ**: ê° ë ˆì´ì–´ì˜ hidden stateë¥¼ ì„ë² ë”©ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥

## ì„¤ì¹˜

```bash
# ì €ì¥ì†Œ í´ë¡ 
cd ELMo_repo

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt
```

## ì‚¬ìš©ë²•

### í•™ìŠµ

```bash
# Depth 1 ëª¨ë¸ í•™ìŠµ
python src/train.py --config configs/elmo_depth-1_seed-42.yaml

# Depth 2 ëª¨ë¸ í•™ìŠµ
python src/train.py --config configs/elmo_depth-2_seed-42.yaml
```

### í‰ê°€

```bash
python src/eval.py \
    configs/elmo_depth-2_seed-42.yaml \
    runs/checkpoints_elmo_depth-2/elmo_depth-2_seed-42.pth \
    data/word_similarity/wordsim353_sim.csv \
    data/word_similarity/SimLex-999.txt \
    data/word_similarity/questions-words.txt \
    --save_csv results/elmo_depth-2_seed-42.csv
```

**ì¶œë ¥ ì˜ˆì‹œ:**
```
ğŸ“Š WordSim-353 Spearman: 0.6285
ğŸ“˜ SimLex-999 Spearman: 0.2639
ğŸ‘‘ Google Analogy Accuracy: 0.3831
```

## ì„¤ì • íŒŒì¼

YAML íŒŒì¼ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤:

```yaml
# configs/elmo_depth-2_seed-42.yaml
vocab_size: 30000
embedding_dim: 512
hidden_dim: 512
num_layers: 2  # depth: 1 or 2
dropout: 0.1
seq_len: 20
batch_size: 32
lr: 0.001
epochs: 1
seed: 42
enable_subsampling: true
subsample_t: 1e-3
num_workers: 16
```

## ëª¨ë¸ êµ¬ì¡°

ELMo ëª¨ë¸ì€ ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°ë¥¼ ê°€ì§‘ë‹ˆë‹¤:

1. **ë‹¨ì–´ ì„ë² ë”© ë ˆì´ì–´**: ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
2. **Forward LSTM**: ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì½ëŠ” ì–¸ì–´ ëª¨ë¸
3. **Backward LSTM**: ì˜¤ë¥¸ìª½ì—ì„œ ì™¼ìª½ìœ¼ë¡œ ì½ëŠ” ì–¸ì–´ ëª¨ë¸
4. **ë‹¤ì¸µ êµ¬ì¡°**: Depth 1 ë˜ëŠ” 2ì˜ LSTM ë ˆì´ì–´

ê° ë ˆì´ì–´ì˜ hidden stateë¥¼ ê²°í•©í•˜ì—¬ ìµœì¢… ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.

## í‰ê°€ í•­ëª©

ELMo ëª¨ë¸ì€ ë‹¤ìŒ í•­ëª©ìœ¼ë¡œ í‰ê°€ë©ë‹ˆë‹¤:

### Intrinsic Evaluation (ë‹¨ì–´ ìˆ˜ì¤€)
- **Depth 1 bi-LSTM**: 1ì¸µ ì–‘ë°©í–¥ LSTM
- **Depth 2 bi-LSTM**: 2ì¸µ ì–‘ë°©í–¥ LSTM

ê° ëª¨ë¸ì€ ë‹¤ìŒ ë°ì´í„°ì…‹ìœ¼ë¡œ í‰ê°€ë©ë‹ˆë‹¤:
- WordSim-353 (ë‹¨ì–´ ìœ ì‚¬ë„)
- SimLex-999 (ë‹¨ì–´ ìœ ì‚¬ë„)
- Google Analogy (ë‹¨ì–´ ìœ ì¶”)

### Extrinsic Evaluation (ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ íƒœìŠ¤í¬)
ELMo ì„ë² ë”©ì„ ì‚¬ìš©í•œ sequence classification tasks:
- **SST-2**: ê°ì • ë¶„ì„ (Sentiment Analysis)
- **MRPC**: ë¬¸ì¥ ìŒ ë¶„ë¥˜ (Paraphrase Detection)
- **CoNLL-03 NER**: ê°œì²´ëª… ì¸ì‹ (Named Entity Recognition)

```bash
# Sequence tasks í‰ê°€
python src/eval_sequence.py \
    configs/elmo_depth-2_seed-42.yaml \
    runs/checkpoints_elmo_depth-2/elmo_depth-2_seed-42.pth \
    --sst2_dir data/sequence_tasks/sst2 \
    --mrpc_dir data/sequence_tasks/mrpc \
    --ner_dir data/sequence_tasks/conll03
```

## í”„ë¡œì íŠ¸ êµ¬ì¡°

```
ELMo_repo/
â”œâ”€â”€ configs/              # ì‹¤í—˜ ì„¤ì • íŒŒì¼ (YAML)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ pretrain/         # í•™ìŠµ ë°ì´í„°
â”‚   â””â”€â”€ word_similarity/  # í‰ê°€ ë°ì´í„°ì…‹
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ model.py          # ELMo ëª¨ë¸
â”‚   â”œâ”€â”€ data.py           # ë°ì´í„° ë¡œë”
â”‚   â”œâ”€â”€ train.py          # í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”‚   â””â”€â”€ eval.py           # í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ runs/
â”‚   â”œâ”€â”€ checkpoints_elmo_depth-1/  # Depth 1 ì²´í¬í¬ì¸íŠ¸
â”‚   â”œâ”€â”€ checkpoints_elmo_depth-2/  # Depth 2 ì²´í¬í¬ì¸íŠ¸
â”‚   â””â”€â”€ metrics/          # í•™ìŠµ ë©”íŠ¸ë¦­
â””â”€â”€ results/              # í‰ê°€ ê²°ê³¼
```

## ì°¸ê³ 

ì´ êµ¬í˜„ì€ Word2Vec_repoì˜ êµ¬ì¡°ë¥¼ ì°¸ê³ í•˜ì—¬ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.

